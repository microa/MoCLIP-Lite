# MoCLIP-Lite: Motion Vector-based Lightweight Video Action Recognition

A lightweight video action recognition framework that combines Motion Vectors (MV) with CLIP for efficient video understanding on the UCF-101 dataset.

## ğŸ¯ Overview

MoCLIP-Lite leverages compressed video motion vectors as a lightweight alternative to traditional RGB-based video analysis, combined with CLIP's powerful text-image understanding capabilities for zero-shot action recognition.

## âœ¨ Key Features

- **Motion Vector Processing**: Efficient extraction and processing of motion vectors from compressed videos
- **Multi-modal Fusion**: Late fusion of motion vector features with CLIP text embeddings
- **Zero-shot Learning**: CLIP-based zero-shot action recognition capabilities
- **Lightweight Architecture**: Based on EfficientNet for efficient inference
- **Comprehensive Evaluation**: Support for both supervised and zero-shot evaluation

## ğŸ—ï¸ Architecture

The framework consists of three main components:

1. **MV-TSN Model**: Motion Vector Temporal Segment Network based on EfficientNet
2. **CLIP Integration**: Pre-trained CLIP model for text-image understanding
3. **Late Fusion**: Combines MV features with CLIP text embeddings for final classification

## ğŸ“‹ Requirements

```bash
torch>=1.9.0
torchvision>=0.10.0
transformers>=4.20.0
timm>=0.6.0
opencv-python>=4.5.0
numpy>=1.21.0
matplotlib>=3.5.0
tqdm>=4.64.0
PIL>=8.3.0
```

## ğŸš€ Installation

1. Clone the repository:
```bash
git clone https://github.com/microa/MoCLIP-Lite.git
cd MoCLIP-Lite
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ“Š Dataset Preparation

1. Download UCF-101 dataset and extract to your data directory
2. Extract motion vectors from videos
3. Update data paths in the configuration files

## ğŸ”§ Usage

### Training MV-TSN Model

```bash
python train_mv_tsn.py --data_root /path/to/ucf101 --mv_root /path/to/motion_vectors
```

### Training Fusion Model

```bash
python train_fusion.py --mv_model_path mv_tsn_best_model.pth --data_root /path/to/ucf101
```

### Zero-shot Evaluation

```bash
python evaluate_zeroshot.py --data_root /path/to/ucf101 --mv_root /path/to/motion_vectors
```

### Testing Final Fusion Model

```bash
python test_fusion_final.py --model_path fusion_best_model.pth --data_root /path/to/ucf101
```

## ğŸ“ Project Structure

```
MoCLIP-Lite/
â”œâ”€â”€ model.py                 # MV-TSN model definition
â”œâ”€â”€ train_mv_tsn.py         # MV model training
â”œâ”€â”€ train_fusion.py         # Fusion model training
â”œâ”€â”€ train_mv_only.py        # MV-only training
â”œâ”€â”€ dataloader.py           # Basic data loader
â”œâ”€â”€ dataloader_coviar.py    # TSN data loader
â”œâ”€â”€ transforms_video.py     # Video transformations
â”œâ”€â”€ mv_quiver.py           # Motion vector visualization
â”œâ”€â”€ mv_to_rgb.py           # MV to RGB conversion
â”œâ”€â”€ evaluate_zeroshot.py   # Zero-shot evaluation
â”œâ”€â”€ test_fusion_final.py   # Fusion model testing
â”œâ”€â”€ test_mv_tsn.py         # MV model testing
â”œâ”€â”€ generate_text_features.py    # Text feature generation
â”œâ”€â”€ precompute_clip_features.py  # CLIP feature precomputation
â”œâ”€â”€ class_mappings.json    # Class name mappings
â”œâ”€â”€ prompt_templates.json  # Text prompt templates
â””â”€â”€ README.md              # This file
```

## ğŸ¨ Visualization

The framework includes tools for visualizing motion vectors and generating qualitative analysis reports:

```bash
python mv_quiver.py --input_dir /path/to/motion_vectors --output_dir /path/to/visualizations
```

## ğŸ“ˆ Results

The model achieves competitive performance on UCF-101 dataset with significantly reduced computational requirements compared to RGB-based methods.

## ğŸ“ Citation

If you use this code in your research, please cite our paper:

```bibtex
@article{moclip_lite_2025,
  title={MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors},
  author={Binhua Huang, Ni Wang, Arjun Pakrashi, Soumyabrata Dev},
  journal={Arxiv},
  year={2025}
}
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- UCF-101 dataset creators
- CLIP model by OpenAI
- EfficientNet by Google Research
- PyTorch and the open-source community
