# MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors

A lightweight video action recognition framework that combines Motion Vectors (MV) with CLIP for efficient video understanding on the UCF-101 dataset.

## ğŸ¯ Overview

MoCLIP-Lite leverages compressed video motion vectors as a lightweight alternative to traditional RGB-based video analysis, combined with CLIP's powerful text-image understanding capabilities for zero-shot action recognition.

## âœ¨ Key Features

- **Motion Vector Processing**: Efficient extraction and processing of motion vectors from compressed videos
- **Multi-modal Fusion**: Late fusion of motion vector features with CLIP text embeddings
- **Zero-shot Learning**: CLIP-based zero-shot action recognition capabilities
- **Lightweight Architecture**: Based on EfficientNet for efficient inference
- **Comprehensive Evaluation**: Support for both supervised and zero-shot evaluation

## ğŸ—ï¸ Architecture

The framework consists of three main components:

1. **MV-TSN Model**: Motion Vector Temporal Segment Network based on EfficientNet
2. **CLIP Integration**: Pre-trained CLIP model for text-image understanding
3. **Late Fusion**: Combines MV features with CLIP text embeddings for final classification

## ğŸ“‹ Requirements

```bash
torch>=1.9.0
torchvision>=0.10.0
transformers>=4.20.0
timm>=0.6.0
opencv-python>=4.5.0
numpy>=1.21.0
matplotlib>=3.5.0
tqdm>=4.64.0
PIL>=8.3.0
```

## ğŸš€ Installation

1. Clone the repository:
```bash
git clone https://github.com/microa/MoCLIP-Lite.git
cd MoCLIP-Lite
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Install the package (optional, for development):
```bash
pip install -e .
```

## ğŸ“Š Dataset Preparation

1. Download UCF-101 dataset and extract to your data directory
2. Extract motion vectors from videos
3. Update data paths in the configuration files

## ğŸ”§ Usage

### Training MV-TSN Model

```bash
python training/train_mv_tsn.py --data_root /path/to/ucf101 --mv_root /path/to/motion_vectors
```

### Training Fusion Model

```bash
python training/train_fusion.py --mv_model_path mv_tsn_best_model.pth --data_root /path/to/ucf101
```

### Zero-shot Evaluation

```bash
python evaluation/evaluate_zeroshot.py --data_root /path/to/ucf101 --mv_root /path/to/motion_vectors
```

### Testing Final Fusion Model

```bash
python evaluation/test_fusion_final.py --model_path fusion_best_model.pth --data_root /path/to/ucf101
```

## ğŸ“ Project Structure

```
MoCLIP-Lite/
â”œâ”€â”€ README.md                    # Project documentation
â”œâ”€â”€ LICENSE                      # MIT License
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ setup.py                     # Package installation script
â”œâ”€â”€ configs/                     # Configuration files
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ class_mappings.json      # Class name mappings
â”‚   â””â”€â”€ prompt_templates.json    # Text prompt templates
â”œâ”€â”€ data/                        # Data processing modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataloader.py            # Basic data loader
â”‚   â”œâ”€â”€ dataloader_coviar.py     # TSN data loader
â”‚   â””â”€â”€ transforms_video.py      # Video transformations
â”œâ”€â”€ models/                      # Model definitions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ model.py                 # MV-TSN model definition
â”œâ”€â”€ training/                    # Training scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train_mv_tsn.py          # MV model training
â”‚   â”œâ”€â”€ train_fusion.py          # Fusion model training
â”‚   â””â”€â”€ train_mv_only.py         # MV-only training
â”œâ”€â”€ evaluation/                  # Evaluation and testing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluate_zeroshot.py     # Zero-shot evaluation
â”‚   â”œâ”€â”€ test_fusion_final.py     # Fusion model testing
â”‚   â”œâ”€â”€ test_mv_tsn.py           # MV model testing
â”‚   â””â”€â”€ test_mv_tsn_32.py        # MV model testing (32 segments)
â”œâ”€â”€ utils/                       # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mv_quiver.py             # Motion vector visualization
â”‚   â”œâ”€â”€ mv_to_rgb.py             # MV to RGB conversion
â”‚   â”œâ”€â”€ generate_text_features.py    # Text feature generation
â”‚   â””â”€â”€ precompute_clip_features.py  # CLIP feature precomputation
â””â”€â”€ scripts/                     # Example scripts
    â”œâ”€â”€ __init__.py
    â””â”€â”€ example_usage.py         # Usage examples
```

## ğŸ¨ Visualization

The framework includes tools for visualizing motion vectors and generating qualitative analysis reports:

```bash
python utils/mv_quiver.py --input_dir /path/to/motion_vectors --output_dir /path/to/visualizations
```

## ğŸ“ˆ Results

The model achieves competitive performance on UCF-101 dataset with significantly reduced computational requirements compared to RGB-based methods.

## ğŸ“ Citation

If you use this code in your research, please cite our paper:

```bibtex
@article{huang2025mocliplite,
  title={MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors},
  author={Huang, Binhua and Wang, Nan and Parakash, Arjun and Dev, Soumyabrata},
  journal={arXiv preprint arXiv:2509.17084},
  year={2025}
}
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- UCF-101 dataset creators
- CLIP model by OpenAI
- EfficientNet by Google Research
- PyTorch and the open-source community
